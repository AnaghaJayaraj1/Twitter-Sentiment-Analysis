{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis with Pyspark \n",
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step in any Apache Spark programming is to create a SparkContext. SparkContext is needed when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster. It is first step to connect with Apache Cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n",
      "SparkContext Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "import warnings\n",
    "\n",
    "SCC_CHECKPOINT_PATH = \"/Users/anujchaudhari/Desktop/256/project/samples/twitter_streaming/checkpoint\"\n",
    "STREAMING_SOCKET_IP = \"192.168.0.100\"\n",
    "STREAMING_SOCKET_PORT = 5555\n",
    "STREAMING_TIME_INTERVAL = 2\n",
    "\n",
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"twitter\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    print(\"Just created a SparkContext\")\n",
    "    \n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Create Spark Streaming Context\n",
    "\n",
    "ssc = StreamingContext(sc, STREAMING_TIME_INTERVAL )\n",
    "ssc.checkpoint(SCC_CHECKPOINT_PATH)\n",
    "socket_stream = ssc.socketTextStream(STREAMING_SOCKET_IP, STREAMING_SOCKET_PORT)\n",
    "lines = socket_stream.window(STREAMING_TIME_INTERVAL)\n",
    "\n",
    "print(\"SparkContext Master: \" + sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data From 'clean_tweet.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('clean_tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+\n",
      "|_c0|                text|target|\n",
      "+---+--------------------+------+\n",
      "|  0|awww that s a bum...|     0|\n",
      "|  1|is upset that he ...|     0|\n",
      "|  2|i dived many time...|     0|\n",
      "|  3|my whole body fee...|     0|\n",
      "|  4|no it s not behav...|     0|\n",
      "+---+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1596753"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully loading the data as Spark Dataframe, we can take a peek at the data by calling .show(), which is equivalent to Pandas .head(). After dropping NA, we have a bit less than 1.6 million Tweets. I will split this in three parts; training, validation, test. Since I have around 1.6 million entries, 1% each for validation and test set will be enough to test the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=212, text='but this is canada canada is weird we re supposed to get snow through wednesday ugh', target=0),\n",
       " Row(_c0=474, text='cant see the flowers falling i dont have a camera just my cellphone', target=0),\n",
       " Row(_c0=488, text='have watched that considering today yaknow shawnna tomomorrow i need my bestfriend', target=0),\n",
       " Row(_c0=544, text='i know exactly how you feel', target=0),\n",
       " Row(_c0=752, text='still procrastinating i hate organizing my clothes there s just so much', target=0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to use VectorAssembler in the pipeline, to combine the features I get from each n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def build_ngrams_wocs(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)\n",
    "predictions_wocs = trigramwocs_pipelineFit.transform(val_set)\n",
    "accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(val_set.count())\n",
    "\n",
    "# print accuracy\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy_wocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Trained Model for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramwocs_pipelineFit.save(\"Model_Twitter_Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "pipeline = PipelineModel.load(\"Model_Twitter_Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8118\n"
     ]
    }
   ],
   "source": [
    "test_predictions = pipeline.transform(val_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(val_set.count())\n",
    "\n",
    "# # print accuracy\n",
    "print(\"Accuracy Score: {0:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
