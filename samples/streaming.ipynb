{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "import pyspark as SparkContext\n",
    "import warnings\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    sc = SparkContext.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    \n",
    "ssc = StreamingContext(sc, 5 )\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc.checkpoint( \"/Users/anujchaudhari/Desktop/256/project/samples/twitter_streaming/checkpoint\")\n",
    "\n",
    "socket_stream = ssc.socketTextStream(\"192.168.0.100\", 5556)\n",
    "\n",
    "lines = socket_stream.window( 5 )\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-04-22 13:50:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-04-22 13:50:10\n",
      "-------------------------------------------\n",
      "['1 :  @kp24: skyâ€™s commentators now cashing in on the @ipl. the same ones that played a major part in ending my england career. ']\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-04-22 13:50:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-04-22 13:50:20\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    try:\n",
    "        # Get the singleton instance of SparkSession\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame\n",
    "#         wordsDataFrame.createOrReplaceTempView(\"text\")\n",
    "\n",
    "#         # Do word count on table using SQL and print it\n",
    "#         wordCountsDataFrame = spark.sql(\"select * from text \")\n",
    "#         wordCountsDataFrame.show()\n",
    "    except:\n",
    "        print(\" exception \")\n",
    "        pass\n",
    "\n",
    "lines = lines.map(lambda x: x.lower());\n",
    "lines = lines.map(lambda x: x.replace(\" rt \" , \" \"))\n",
    "lines = lines.map(lambda x: x.replace(\"\\n\" , \" \"))\n",
    "lines = lines.reduce(lambda x,y : x + y)\n",
    "lines = lines.map(lambda x: x.split(\" $$$$$$ \"))\n",
    "# lines = lines.map(lambda x: tweet_cleaner_updated(x))\n",
    "\n",
    "#df = sqlContext.createDataFrame(lines.collect(), [\"text\"])\n",
    "\n",
    "#lines.foreachRDD(process)\n",
    "\n",
    "#lines.saveAsTextFiles(\"x.csv\")\n",
    "lines.pprint()\n",
    "\n",
    "ssc.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
